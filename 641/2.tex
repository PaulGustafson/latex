\documentclass{article}
\usepackage{../m}
\usepackage[pdftex]{graphicx}

\begin{document}
\noindent Paul Gustafson\\
\noindent Texas A\&M University - Math 641\\ 
\noindent Instructor - Fran Narcowich

\subsection*{HW 2}
\p{1 a} Show that if $\alpha$, $\beta$ are positive with $\alpha + \beta = 1$ then for all $u,v \ge 0$ we have
$$ u^\alpha v^\beta \le \alpha u + \beta v.$$
\begin{proof} If $u = v = 0$, then the inequality holds. Since the inequality is symmetric in $u$ and $v$, we may assume $v \neq 0$.
Hence we wish to show
$$(\frac u v)^\alpha \le \alpha (\frac u v) + \beta$$.
Letting $x = \frac u v$, this is equivalent to showing that $f(x) \ge 0$, where $f(x) = \alpha x -  x^\alpha + \beta$ and $x \ge 0$.  
Since $\alpha > 0$, we have $f'(x) = \alpha - \alpha x^{\alpha - 1} = \alpha (1 - x^{\alpha - 1})$ whose only zero in $[0,\infty)$ is at $x = 1$.
Moreover, since $\alpha < 1$, we have $f''(1) = \alpha (\alpha - 1) x^{\alpha - 2}|_{x = 1} = \alpha (\alpha - 1)  < 0$ . Hence, the maximum value of $f$ on $[0, \infty)$ occurs at $x = 1$.
We have $f(1) = \alpha -1 + \beta = 0$, so $f(x) \le 0$ for $x \ge 0$.
\end{proof}

\p{1 b} Let $x,y \in \R^n$, and let $p > 1$ and define $q$ by $q^{-1} = 1 - p^{-1}$. Prove H\"{o}lder's inequality, 
$$|\sum_j x_j y_j| â‰¤ \|x\|_p \|y\|_q. $$
Hint: Using the inequality in part (a). first prove it for $\|x\|_p = \|y\|_q = 1$. Scale to get the final inequality.
\begin{proof}
Suppose $\|x\|_p = \|y\|_q = 1$. Then
\begin{align*}
| \sum_j x_j y_j | & \le & \sum_j |x_j| |y_j|
\\ &  = &  \sum_j (|x_j|^p)^{1/p} ((|y_j|)^q)^{1/q}
\\ & \le & \sum_j \frac 1 p |x_j|^p + \frac 1 q |y_j|^q
\\ & \le & \frac 1 p (\sum_j  |x_j|^p) + \frac 1 q (\sum_j |y_j|^q)
\\ & = & \frac 1 p + \frac 1 q
\\ & = & 1.
\end{align*}

For the general case, note that if $x = 0$ or $y = 0$ then the inequality holds. Hence we may assume both are nonzero.  
Let $x' = \frac x \|x\|_p$ and $y' = \frac y \|y\|_p$.  We can now apply the special case to $x'$ and $y'$ then clear denominators to get the general inequality.
\end{proof}


\p{1 c} Suppose $\phi =(y_1, \ldots, y_n) \in l_p^*$. H\"{o}lder's inequality implies that $\|\phi\|_{l_p^*} \le \|y\|_q$. Show that we actually have $\|\phi\|_{l_p^*}  = \|y \|_q$.
\begin{proof}
%(y_i/|y|_q)^q = (x_i/|x|_p)^p
If $\|y\|_q = 0$ then $\phi = 0$, and $\|\phi\|_{l_p^*} = 0 = \|y\|_q$.  Hence, we may assume $\|y\|_q \neq 0$.
Let $x_i = \sign(y_i) \frac {|y_i|^{q/p}}{\|y\|_q^{q/p}}$ for $1 \le i \le n$.  Then $\|x\|_p = \sum_i \frac {|y_i|^q} {\|y\|_q^q} = 1$.

Then $\phi(x) = \sum_i x_i y_i = \sum_i \frac {|y_i|^{q/p}}{\|y\|_q^{q/p}} |y_i| = \frac 1 {\|y\|_q^{q/p}} \sum_i |y_i|^{\frac {p + q} p }  = \frac 1 {\|y\|_q^{q/p}} \sum |y_i|^q = \|y\|_q^{q - q/p} = \|y\|_q$.

\end{proof}

\p{1 d} Let $x,y \in \R^n$, and let $p > 1$. Prove Minkowski's inequality, 
$$\|x + y \|_p \le \|x\|_p + \|y\|_p. $$
Use this to show that $\|x\|_p$ defines a norm on $\R^n$. Hint: you will need to use H\"{o}lder's inequality, along with a trick.

\begin{proof}
Acknowledgement: I looked in Carother's Real Analysis book for a hint on this problem. Setting $1/p + 1/q = 1$, we have 
\begin{align*}
\|x + y \|_p^p & = & \sum_i |x_i + y_i| |x_i + y_i|^{p-1}
\\ & \le & \sum_i |x_i| |x_i + y_i|^{p-1} + \sum_i |y_i| |x_i + y_i|^{p-1}
\\ & \le & ( \|x\|_p + \|y\|_p) \left( \sum_i |x_i + y_i|^{q(p-1)} \right)^{1/q}
\\ & = & ( \|x\|_p + \|y\|_p) \left( \sum_i |x_i + y_i|^p \right)^{1 - 1/p}
\\ & = &  ( \|x\|_p + \|y\|_p) \|x + y \|_p^{p - 1}
\end{align*}

If $\|x + y\|_p^{p-1} \neq 0$, we can divide by it to get desired inequality. If $\|x + y \|_p = 0$ then the inequality follows from the fact that $\|x\|_p + \|y\|_p$ must be nonnegative by definition.

To show that $\| \cdot \|_p$ is a norm, it remains to show that it is homogeneous and positive definite. To see that $\| \cdot \|_p$ is homogeneous, let $c \in \R$ and $v \in \R^n$, then $\|c v\|_p = \left( \sum_i |c v_i|^p \right)^{1/p} = \left(|c|^p \sum_i | v_i|^p \right)^{1/p} = |c| \|v\|_p$. It is obvious that $\|v\|_p \ge 0$.  If $\|v\|_p = 0$, then each component of $v$ must be zero or else $\sum_i |v_i|^p > 0$. Hence $v = 0$.

\end{proof}



\p{2} $L_2$ minimization. Find the straight line $y = a + bx$ that minimizes $\int_0^1(e^{-x} - a - bx)^2 \, dx$.
\begin{proof}
By HW 1, Problem 4, we know that $a + bx$ minimizes $\|e^{-x} - a - bx\|_2$ iff 
\begin{align*}
\left( \begin{array}{c}
\langle e^{-x}, 1 \rangle \\
\langle e^{-x}, x \rangle  \end{array}\right)
& = &
\left( \begin{array}{cc}
\langle 1, 1 \rangle & \langle x , 1 \rangle \\
\langle 1, x \rangle & \langle x , x \rangle \end{array} \right) 
\left( \begin{array}{c}
a \\
b \end{array}\right) 
\end{align*}

The one slightly tricky integral is $\langle e^{-x} , x \rangle = \int_0^1 x e^{-x} \, dx = x (-e^{-x}) \rvert_{x=0}^1 + \int_0^1 e^{-x} \, dx =
-e^{-1} - (e^{-x}) \rvert_{x=0}^1 =  -e^{-1} - (e^{-1} - 1) = 1 - 2e^{-1}$.

\begin{align*}
\left( \begin{array}{c}
 1 - e^{-1}  \\
 1 - 2e^{-1}  \end{array}\right) & = & \left( \begin{array}{cc}
                                     1 &  1/2  \\
                                     1/2 & 1/3 \end{array} \right) 
                                                                  \left( \begin{array}{c}
                                                                      a \\
                                                                      b \end{array}\right) 
\\ \left( \begin{array}{c}
 a  \\
 b  \end{array}\right) & = & \left( \begin{array}{c}
                                0.943036 \\
                                -0.62183  \end{array}\right)       
\end{align*} 

\end{proof}

\p{3} $L_1$ minimization. Find the straight line $y = a + bx$ that minimizes  $\int_0^1|e^{-x} - a - bx| \, dx$, by following these steps.

a. Whatever the minimizer is, geometric considerations show that $e^{-x}$ and $a+bx$ will cross at two points, $0 < s < t < 1$. Find these two points by minimizing, over $a,b$, the area $A$ between $f(x)$ and $a+bx$:
$$A=\int_0^1 |e^{-x} - a -bx| \, dx = \int_0^s (e^{-x} - a -bx)\,dx + \int_s^t (a+bx-e^{-x}) \,dx +  \int_t^1(e^{-x} - a - bx) \,dx.$$

b. Use the crossing conditions $a + bs = e^{-s}$ and $a + bt = e^{-t}$ to find $a$ and $b$.

\begin{proof}

a. Let $g_1(a,b,s) = e^{-s} - a - bs$, and $g_2(a,b,t) = e^{-t} - a - bt$. Geometric considerations imply that the global minimum of $A$ with the constraint $g_1 = g_2 = 0$ must be a local minimum, not a boundary value. Hence, Lagrange multipliers gives us the following necessary condition for $(a,b,s,t)$ to minimize $A$ given the constraints $g_1 = g_2 = 0$:
\begin{align}
 0      & = &  (\pder a, \pder b, \pder s, \pder t) (A - \lambda_1 g_1 - \lambda_2 g_2) \nonumber
\\ \nonumber
\\ 0     & = & ( \int_0^s  (-1) \,dx + \int_s^t 1 \,dx + \int_t^1 (-1) \, dx  - \lambda_1 - \lambda_2 , \nonumber
\\ & &                \int_0^s  (-x) \,dx + \int_s^t x \,dx + \int_t^1 (-x) \, dx  - \lambda_1 s - \lambda_2 t, \nonumber
\\ & &           2(-e^{-s} - a -bs)  + \lambda_1(-e^{-s} - b),
                - 2(-e^{-t} - a - bt) + \lambda(-e^{-t} - b)) \nonumber
\\ \nonumber
\\  0    & = & ( -s + (t - s) + (t - 1) - \lambda_1 - \lambda_2, \nonumber
\\ & &            (-1/2)s^2  + (1/2)(t^2 - s^2)  + (-1/2) (1 - t^2) - \lambda_1 s - \lambda_2 t, \nonumber
\\ & &          \lambda_1(-e^{-s} - b),
                \lambda_2(-e^{-t} - b)) \nonumber
\\ \nonumber
\\ 0    & = & ( 2t - 2s - 1 - \lambda_1 - \lambda_2, t^2 - s^2 - 1/2  - \lambda_1 s - \lambda_2 t, \nonumber
\\ & &  \lambda_1(-e^{-s} - b), \lambda_2(-e^{-t} - b) ) \label{eq1}
\end{align}
From the last two components, we get four cases.

\emph{Case $e^{-s} = e^{-t} = -b$.} Since $b$ is the slope of the line between $(s, e^{-s})$ and $(t, e^{-t})$, we have $b = \frac {e^{-t} - e^{-s}}{t - s} = 0$ which cannot correspond to a minimum.

\emph{Case $e^{-s} = -b$ and $\lambda_2 = 0$.} From the first component of \ref{eq1}, we have
$\lambda_1 = 2t - 2s -1$. Substituting into the second component of \ref{eq1}, 
$0 = t^2 - s^2 - 1/2  - \lambda_1 s = t^2 - s^2 - 1/2 - (2t - 2s -1)s = (t - s)^2 - (1/2 -s)$. Since $t - s > 0$, we have 
$t = s + \sqrt{1/2 -s}$.
Using the case assumption, we have $e^{-s} = -b = -\frac{e^{-t} - e^{-s}}{t-s} = - e^{-s} \frac {e^{-\sqrt{1/2 - s}} -1} {\sqrt{1/2 - s}}$. Thus if $u = -\sqrt{1/2 - s}$,
then $u = e^u -1$.  The only solution to this equation is $u = 0$. To see this, note that $f(u) := e^u - u - 1$ has derivative $e^u -1$, hence $f$ has a unique global minimum at $0$.

Hence $s = 1/2$, so $t = 1/2$. This cannot correspond to a minimum.

\emph{Case $\lambda_1 = 0$ and $e^{-t} = -b$.} From the first component of \ref{eq1}, we have
$\lambda_2 = 2t - 2s -1$. Substituting into the second component of \ref{eq1}, 
$0 = t^2 - s^2 - 1/2  - \lambda_2 t = t^2 - s^2 - 1/2 - (2t - 2s -1)t = -(t - s)^2 + t - 1/2$. Since $t -s > 0$, we have $s = t - \sqrt{t - 1/2}$.
Using the case assumption, we have $e^{-t} = -b = -\frac{e^{-t} - e^{-s}}{t-s} = -e^{-t} {1 - e^{\sqrt{t- 1/2 }}} {\sqrt{t - 1/2}}$. Thus if $u = \sqrt{t - 1/2}$,
then $u = e^u - 1$.  As in the previous case, the only solution to this equation is $u = 0$. 

Hence $t = 1/2$, so $s = 1/2$. This cannot correspond to a minimum.

\emph{Case $\lambda_1 = \lambda_2 = 0$.} We have $t = s + 1/2$, so $0 = (s + 1/2)^2 - s^2 -1/2 = s - 1/4$. Hence $s = 1/4, t = 3/4$.

b. We have $a + b(1/4) = e^{-(1/4)}$ and $a + b(3/4) = e^{-(3/4)}$.  Hence $a = 0.9320$ and $b = -0.6128$.

\end{proof}

\p{3} Use your favorite software (mine is Matlab) and plot, on the same set of axes, $e^{-x}$ and the two minimization solutions found in the previous two problems.

% using Winston; 
% a1 = inv([1 1/4; 1 3/4])* [exp(-1/4); exp(-3/4)]
% a2 = inv([1  1/2; 1/2  1/3]) * [ 1 - exp(-1) ;  1 - 2*exp(-1)]
% x = [0:0.01:1];
% c0 = Curve(x, exp(-x));
% setattr(c0, "label", "exp(-x)");
% c1 = Curve(x, a1[1] + a1[2]*x);
% setattr(c1, "label", "L_1 approx");
% c2 = Curve(x, a2[1] + a2[2]*x);
% setattr(c2, "label", "L_2 approx");
% style(c1, "type", "dotted");
% style(c2, "type", "dashed");
% p = FramedPlot();
% l = Legend( .5, .9, {c0, c1, c2} );
% add(p, c0, c1, c2, l);
% Winston.display(p);
% file(p, "approx.png");


\includegraphics[height = 3in] {approx.png}


\p{4} Let $V$ be a finite dimensional inner product space and let $U$ be a subspace of $V$. Recall that the orthogonal complement of $U$ is 
$$U^\perp = \{v \in V | \langle v,u \rangle = 0 \text{ for all u } \in U\}.$$
Show that $V = U \oplus U^\perp$, where $\oplus$ symbolizes the direct sum of vector spaces. Also, show that $(U^\perp)^\perp = U$.
\begin{proof}
By HW 1 (4)(b), the orthogonal projection $P : V \to U$ exists.  Let $v \in V$. Then $v = Pv + (v - Pv)$. By HW 1 (3), $v - Pv \in U^\perp$. Hence, $V = U + U^\perp$.  Moreover, if $w \in U \cap U^\perp$, then $\langle w, w \rangle = 0$ so $w = 0$. Thus, $v = U \oplus U^\perp$.

To see that $U \subset (U^\perp)^\perp$, let  $u \in U$. Then $\langle v,u \rangle = 0$ for all $v \in U^\perp$. Hence, $\langle u,v \rangle = 0$ for all $v \in U^\perp$. Thus, $u \in (U^\perp)^\perp$.

Since $V = W \oplus W^\perp$ for any subspace $W$, we have $\dim (U) + \dim(U^\perp) = \dim (V) = \dim (U^\perp) + \dim((U^\perp)^\perp)$. Since $\dim(U^\perp) < \infty$, we have $\dim(U) = \dim((U^\perp)^\perp)$.  Since $U \subset (U^\perp)^\perp$ and they are finite dimensional, this implies that $U = (U^\perp)^\perp$.

\end{proof}


\end{document}
